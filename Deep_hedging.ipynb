{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46fc736-a7fb-4e69-b4d8-73092bca1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kstest, norm\n",
    "from scipy.stats import kurtosis, skew\n",
    "from statsmodels.tsa.stattools import acf \n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5546ad3c-25fb-40f2-94d9-2ea701cfe24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"AMZN1.csv\")  \n",
    "df2 = pd.read_csv(\"AAPL.csv\")  \n",
    "\n",
    "df1['Date'] = pd.to_datetime(df1['Date'], dayfirst=True, errors='coerce')\n",
    "df1 = df1[(df1['Date'].dt.year > 2010) & (df1['Date'].dt.year < 2021)]\n",
    "df1 = df1.sort_values(by='Date')\n",
    "df2['Date'] = pd.to_datetime(df2['Date'], dayfirst=True, errors='coerce')\n",
    "df2 = df2[(df2['Date'].dt.year > 2010) & (df2['Date'].dt.year < 2021)]\n",
    "df2 = df2.sort_values(by='Date')\n",
    "\n",
    "df1['Day'] = (df1['Date'] - df1['Date'].min()).dt.days\n",
    "df1 = df1.sort_values(by = 'Day')\n",
    "df1['log_return'] = np.log(df1['Adjusted Close']).diff()\n",
    "df1.dropna(inplace=True) \n",
    "df2['Day'] = (df2['Date'] - df2['Date'].min()).dt.days\n",
    "df2 = df2.sort_values(by = 'Day')\n",
    "df2['log_return'] = np.log(df2['Adjusted Close']).diff()\n",
    "df2.dropna(inplace=True) \n",
    "\n",
    "def make_log_return_df(a, n=1000, seed=42):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    mean = -a**2 / 2\n",
    "    std = a\n",
    "    log_returns = np.random.normal(loc=mean, scale=std, size=n)\n",
    "    df = pd.DataFrame({'log_return': log_returns})\n",
    "    return df\n",
    "\n",
    "df1_fake = make_log_return_df(a = 0.5, n = 2000)\n",
    "df2_fake = make_log_return_df(a = 0.05, n = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810a949b-3fbb-4f5a-bdb0-ecd811ce8d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X1 (lagged inputs y_t): torch.Size([2514, 2])\n",
      "Shape of y1 (target r_{t+1}): torch.Size([2514])\n",
      "X1_train shape: torch.Size([1508, 2])\n",
      "y1_train shape: torch.Size([1508])\n",
      "X1_val shape: torch.Size([503, 2])\n",
      "y1_val shape: torch.Size([503])\n",
      "X1_test shape: torch.Size([503, 2])\n",
      "y1_test shape: torch.Size([503])\n",
      "\n",
      "Shape of X2 (lagged inputs y_t): torch.Size([2514, 2])\n",
      "Shape of y2 (target r_{t+1}): torch.Size([2514])\n",
      "X2_train shape: torch.Size([1508, 2])\n",
      "y2_train shape: torch.Size([1508])\n",
      "X2_val shape: torch.Size([503, 2])\n",
      "y2_val shape: torch.Size([503])\n",
      "X2_test shape: torch.Size([503, 2])\n",
      "y2_test shape: torch.Size([503])\n"
     ]
    }
   ],
   "source": [
    "def prepare_lagged_data(df, col='log_return', num_lags=2):\n",
    "    \n",
    "    series = df[col].dropna().values\n",
    "    N = len(series)\n",
    "\n",
    "    X = np.stack([series[i:N - num_lags + i] for i in range(num_lags)], axis=1)\n",
    "    y = series[num_lags:]\n",
    "\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X1, y1 = prepare_lagged_data(df1, col='log_return', num_lags=2)\n",
    "X2, y2 = prepare_lagged_data(df2, col='log_return', num_lags=2)\n",
    "\n",
    "X1_temp, X1_test, y1_temp, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42,shuffle=True)\n",
    "X2_temp, X2_test, y2_temp, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42,shuffle=True)\n",
    "\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1_temp, y1_temp, test_size=0.25, random_state=42,shuffle=True)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2_temp, y2_temp, test_size=0.25, random_state=42,shuffle=True)\n",
    "\n",
    "print(f\"\\nShape of X1 (lagged inputs y_t): {X1.shape}\") \n",
    "print(f\"Shape of y1 (target r_{{t+1}}): {y1.shape}\")\n",
    "print(f\"X1_train shape: {X1_train.shape}\")\n",
    "print(f\"y1_train shape: {y1_train.shape}\")\n",
    "print(f\"X1_val shape: {X1_val.shape}\")\n",
    "print(f\"y1_val shape: {y1_val.shape}\")\n",
    "print(f\"X1_test shape: {X1_test.shape}\")\n",
    "print(f\"y1_test shape: {y1_test.shape}\")\n",
    "\n",
    "print(f\"\\nShape of X2 (lagged inputs y_t): {X2.shape}\") \n",
    "print(f\"Shape of y2 (target r_{{t+1}}): {y2.shape}\")\n",
    "print(f\"X2_train shape: {X2_train.shape}\")\n",
    "print(f\"y2_train shape: {y2_train.shape}\")\n",
    "print(f\"X2_val shape: {X2_val.shape}\")\n",
    "print(f\"y2_val shape: {y2_val.shape}\")\n",
    "print(f\"X2_test shape: {X2_test.shape}\")\n",
    "print(f\"y2_test shape: {y2_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6b3488-ae5e-4248-8d9a-3abc62611e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(r_true, nu):\n",
    "    var = nu**2 + 1e-8\n",
    "    mean = -0.5 * var\n",
    "    return 0.5 * torch.log(2 * torch.pi * var) + ((r_true - mean)**2) / (2 * var)\n",
    "\n",
    "def compute_latent_z(r_true, nu):\n",
    "    return (r_true + 0.5 * nu**2) / (nu + 1e-8)\n",
    "\n",
    "def get_z(model, X, Y):\n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        sigma = model(X_t).squeeze().numpy()\n",
    "    mu = -0.5 * sigma**2\n",
    "    z = (Y - mu) / sigma\n",
    "    return z\n",
    "    \n",
    "def compute_pred(model, X):\n",
    "    with torch.no_grad():\n",
    "        nu = model(X)\n",
    "    nu = np.asarray(nu)\n",
    "    z = np.random.normal(0, 1, size=nu.shape)\n",
    "    return nu * z - 0.5 * nu**2\n",
    "\n",
    "def summarize_distribution(data):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # Basic stats\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    skewness = skew(data)\n",
    "    kurt = kurtosis(data, fisher=True)  # 0 = normal\n",
    "    q25, q50, q75 = np.percentile(data, [25, 50, 75])\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"  Mean      : {mean:.6f}\")\n",
    "    print(f\"  Std Dev   : {std:.6f}\")\n",
    "    print(f\"  Skewness  : {skewness:.6f}\")\n",
    "    print(f\"  Kurtosis  : {kurt:.6f}\")\n",
    "    print(f\"  Quantiles : 25%={q25:.4f}, 50%={q50:.4f}, 75%={q75:.4f}\")\n",
    "\n",
    "\n",
    "class RealizedVolNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.net(x)\n",
    "        nu = torch.exp(a).squeeze()  # Î½_t > 0\n",
    "        return nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f715acc9-e22f-42fb-ae7b-5e552f9b65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_ks_tracking(X_train, y_train, X_val, y_val,\n",
    "                           input_dim, lr=1e-3, epochs=500):\n",
    "    model = RealizedVolNet(input_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Condition tracking\n",
    "    consecutive_count = 0\n",
    "    model_saved = False\n",
    "\n",
    "    # Logs\n",
    "    train_losses, val_losses, train_losses_ND = [], [], []\n",
    "    train_pvalues, val_pvalues = [], []\n",
    "    nus_train_ND = []\n",
    "    nus_val = []\n",
    "    for epoch in range(epochs):\n",
    "        # === Train with dropout ===\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        nu_train = model(X_train)\n",
    "        loss = torch.mean(nll_loss(y_train, nu_train))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # === Evaluate with dropout OFF ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            nu_train_ND = model(X_train)\n",
    "            nus_train_ND.append(torch.mean(nu_train_ND).item())\n",
    "            loss_train_ND = torch.mean(nll_loss(y_train, nu_train_ND)).item()\n",
    "\n",
    "            nu_val = model(X_val)\n",
    "            nus_val.append(torch.mean(nu_val).item())\n",
    "            loss_val = torch.mean(nll_loss(y_val, nu_val)).item()\n",
    "\n",
    "            # Compute latent z for KS test\n",
    "            z_train = compute_latent_z(y_train, nu_train_ND).cpu().numpy()\n",
    "            z_val = compute_latent_z(y_val, nu_val).cpu().numpy()\n",
    "\n",
    "            ks_train = kstest(z_train, 'norm').pvalue\n",
    "            ks_val = kstest(z_val, 'norm').pvalue\n",
    "\n",
    "        # === Record metrics ===\n",
    "        train_losses.append(loss.item())\n",
    "        train_losses_ND.append(loss_train_ND)\n",
    "        val_losses.append(loss_val)\n",
    "        train_pvalues.append(ks_train)\n",
    "        val_pvalues.append(ks_val)\n",
    "        \n",
    "        if (epoch+1 ) % 50 == 0:\n",
    "            print(f\"Epoch {epoch:03d}: \"\n",
    "              f\"Train NLL = {loss.item():.4f}, \"\n",
    "              f\"Train ND NLL = {loss_train_ND:.4f}, \"\n",
    "              f\"Val NLL = {loss_val:.4f}, \"\n",
    "              f\"KS p (train) = {ks_train:.4f}, \"\n",
    "              f\"KS p (Val) = {ks_val:.4f}\")\n",
    "\n",
    "        if epoch > 300:\n",
    "            if loss_train_ND < loss_val:\n",
    "                consecutive_count += 1\n",
    "                if consecutive_count >= 10 and not model_saved:\n",
    "                    print(f\"\\nSaving model at epoch {epoch} \"\n",
    "                          f\"(train_loss_ND < test_loss for 5 consecutive epochs)\")\n",
    "                    print(f\"Epoch {epoch:03d}: \"\n",
    "                      f\"Train NLL = {loss.item():.4f}, \"\n",
    "                      f\"Train ND NLL = {loss_train_ND:.4f}, \"\n",
    "                      f\"Val NLL = {loss_val:.4f}, \"\n",
    "                      f\"KS p (train) = {ks_train:.4f}, \"\n",
    "                      f\"KS p (Val) = {ks_val:.4f}\")\n",
    "                    torch.save(model.state_dict(), \"model_trainND_better_than_test3.pth\")\n",
    "                    model_saved = True\n",
    "            else:\n",
    "                consecutive_count = 0\n",
    "\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_losses_ND': train_losses_ND,\n",
    "        'val_losses': val_losses,\n",
    "        'train_pvalues': train_pvalues,\n",
    "        'val_pvalues': val_pvalues,\n",
    "        'nus_train_ND': nus_train_ND,\n",
    "        'nus_val' : nus_val\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65bad952-5a8e-49f8-b56e-37891b6f59a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049: Train NLL = -2.1475, Train ND NLL = -2.2420, Val NLL = -2.2296, KS p (train) = 0.0000, KS p (Val) = 0.0000\n",
      "Epoch 099: Train NLL = -2.4590, Train ND NLL = -2.5217, Val NLL = -2.4811, KS p (train) = 0.0000, KS p (Val) = 0.0000\n",
      "Epoch 149: Train NLL = -2.4267, Train ND NLL = -2.5274, Val NLL = -2.4830, KS p (train) = 0.0000, KS p (Val) = 0.0000\n",
      "Epoch 199: Train NLL = -2.4384, Train ND NLL = -2.5276, Val NLL = -2.4812, KS p (train) = 0.0000, KS p (Val) = 0.0000\n"
     ]
    }
   ],
   "source": [
    "model1, history1 = train_with_ks_tracking(X1_train, y1_train, X1_val, y1_val,\n",
    "                                        input_dim=X1_train.shape[1], lr=1e-3, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990ca1d0-2761-40a0-8133-a4e63e858a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049: Train NLL = -1.1902, Train ND NLL = -1.2719, Val NLL = -1.2700, KS p (train) = 0.0000, KS p (Val) = 0.0000\n",
      "Epoch 099: Train NLL = -2.4985, Train ND NLL = -2.6109, Val NLL = -2.6125, KS p (train) = 0.0000, KS p (Val) = 0.0000\n",
      "Epoch 149: Train NLL = -2.5093, Train ND NLL = -2.5976, Val NLL = -2.5978, KS p (train) = 0.0000, KS p (Val) = 0.0000\n",
      "Epoch 199: Train NLL = -2.4797, Train ND NLL = -2.6082, Val NLL = -2.6084, KS p (train) = 0.0000, KS p (Val) = 0.0000\n"
     ]
    }
   ],
   "source": [
    "model2, history2 = train_with_ks_tracking(X2_train, y2_train, X2_val, y2_val,\n",
    "                                        input_dim=X2_train.shape[1], lr=1e-3, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832a0ec1-2cf9-4871-9d64-71f836c8c0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated innovation correlation: 0.433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_59192\\3399346040.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_t = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "z1 = get_z(model1, X1, y1)\n",
    "z2 = get_z(model2, X2, y2)\n",
    "rho = np.corrcoef(z1, z2)[0,1]\n",
    "print(f\"Estimated innovation correlation: {rho:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9332afff-e935-4677-9eea-db0c7b46765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lags = 2\n",
    "init_lags_1 = df1['log_return'].dropna().values[-num_lags:]\n",
    "init_lags_2 = df2['log_return'].dropna().values[-num_lags:]\n",
    "\n",
    "def simulate_joint_path(model1, model2, start_lags1, start_lags2, n_steps, rho):\n",
    "    path1 = list(start_lags1)\n",
    "    path2 = list(start_lags2)\n",
    "    num_lags = len(start_lags1)\n",
    "    cov = np.array([[1.0, rho], [rho, 1.0]])\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Sample joint innovations\n",
    "        z = np.random.multivariate_normal([0, 0], cov)\n",
    "        \n",
    "        # Prepare lag window tensors\n",
    "        x1 = torch.tensor(path1[-num_lags:], dtype=torch.float32).unsqueeze(0)\n",
    "        x2 = torch.tensor(path2[-num_lags:], dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sigma1 = model1(x1).item()\n",
    "            sigma2 = model2(x2).item()\n",
    "        \n",
    "        r1 = -0.5 * sigma1 ** 2 + sigma1 * z[0]\n",
    "        r2 = -0.5 * sigma2 **d 2 + sigma2 * z[1]\n",
    "        \n",
    "        path1.append(r1)\n",
    "        path2.append(r2)\n",
    "    \n",
    "    return np.array(path1), np.array(path2)\n",
    "\n",
    "def batch_simulate_joint_paths(model1, model2, X1, X2, n_steps, rho, n_mc=1):\n",
    "\n",
    "    all_sim_r1 = []\n",
    "    all_sim_r2 = []\n",
    "    for i in range(X1.shape[0]):\n",
    "        start1 = X1[i].numpy() if hasattr(X1[i], 'numpy') else X1[i]\n",
    "        start2 = X2[i].numpy() if hasattr(X2[i], 'numpy') else X2[i]\n",
    "        for _ in range(n_mc):\n",
    "            sim_r1, sim_r2 = simulate_joint_path(model1, model2, start1, start2, n_steps, rho)\n",
    "            # remove the initial lags, only keep simulated future\n",
    "            all_sim_r1.append(sim_r1[len(start1):])\n",
    "            all_sim_r2.append(sim_r2[len(start2):])\n",
    "    return np.stack(all_sim_r1), np.stack(all_sim_r2)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "n_steps = 3        # Length of each simulated path\n",
    "n_mc = 1            # Number of simulations per lagged window\n",
    "\n",
    "sim_y1, sim_y2 = batch_simulate_joint_paths(model1, model2, X1, X2, n_steps, rho, n_mc=n_mc)\n",
    "\n",
    "sim_y1 = sim_y1.flatten()\n",
    "sim_y2 = sim_y2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33e39c74-253e-42c3-8ccc-4a9eac4eb8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real return correlation: 0.4423793186554475\n",
      "Simulated return correlation: 0.4491410629651049\n"
     ]
    }
   ],
   "source": [
    "print(\"Real return correlation:\", np.corrcoef(y1[:len(y2)], y2)[0,1])\n",
    "print(\"Simulated return correlation:\", np.corrcoef(sim_y1[num_lags:], sim_y2[num_lags:])[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b4336-547a-44b5-ae2e-fef13ed35218",
   "metadata": {},
   "source": [
    "# Using one of the dataset we are going to hedge the payoff of a 5 day call option "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9171c573-ba48-46e0-a99b-e5563e58e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns_to_spot_path(S_init, sim_returns):\n",
    "    growth_factors = np.exp(sim_returns)\n",
    "    return S_init * np.cumprod(np.concatenate([[1], growth_factors]))\n",
    "\n",
    "def sim_paths_hedge(n_steps, start_lags1, start_lags2, S1_0,S2_0):\n",
    "    sim_path1, sim_path2 = simulate_joint_path(model1, model2, start_lags1, start_lags2, n_steps, rho)\n",
    "    sim_returns1 = sim_path1[2:]\n",
    "    sim_spot1_path = returns_to_spot_path(S1_0, sim_returns1)\n",
    "    sim_returns2 = sim_path2[2:]\n",
    "    sim_spot2_path = returns_to_spot_path(S2_0, sim_returns2)\n",
    "    return sim_spot1_path,sim_spot2_path\n",
    "\n",
    "S1_0 = df1['Adjusted Close'].iloc[-6]\n",
    "S2_0 = df2['Adjusted Close'].iloc[-6]\n",
    "start_lags1 = [y1[-7], y1[-6]]  \n",
    "start_lags2 = [y2[-7], y2[-6]]  \n",
    "n_steps = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1b732-c2dd-42ff-8572-19a082dafbe8",
   "metadata": {},
   "source": [
    "# Own RL Hedging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "397143f7-5218-49ba-ac4f-7dc785f3f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketHedgingEnv:\n",
    "    def __init__(self, S1_path, S2_path, strike, w1=0.5, w2=0.5, cost=0.0, include_prev_hedge=True):\n",
    "        self.S1_path = S1_path\n",
    "        self.S2_path = S2_path\n",
    "        self.strike = strike\n",
    "        self.n_steps = len(S1_path) - 1\n",
    "        self.cost = cost\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.include_prev_hedge = include_prev_hedge\n",
    "        self.reset()\n",
    "\n",
    "    def _get_state(self):\n",
    "        S1_norm = self.S1 / self.S1_path[0]\n",
    "        S2_norm = self.S2 / self.S2_path[0]\n",
    "        time_norm = (self.n_steps - self.t) / self.n_steps\n",
    "        if self.include_prev_hedge:\n",
    "            state = [S1_norm, S2_norm, self.hedge1, self.hedge2, time_norm]\n",
    "        else:\n",
    "            state = [S1_norm, S2_norm, time_norm]\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.S1 = self.S1_path[self.t]\n",
    "        self.S2 = self.S2_path[self.t]\n",
    "        self.hedge1 = 0.0\n",
    "        self.hedge2 = 0.0\n",
    "        self.cash = 0.0\n",
    "        self.history = [(self.S1, self.S2, self.hedge1, self.hedge2, self.cash)]\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_hedge1, prev_hedge2 = self.hedge1, self.hedge2\n",
    "        delta_hedge1 = action[0] - prev_hedge1\n",
    "        delta_hedge2 = action[1] - prev_hedge2\n",
    "\n",
    "        \n",
    "        self.cash -= delta_hedge1 * self.S1\n",
    "        self.cash -= delta_hedge2 * self.S2\n",
    "\n",
    "       \n",
    "        self.cash -= abs(delta_hedge1) * self.S1 * self.cost\n",
    "        self.cash -= abs(delta_hedge2) * self.S2 * self.cost\n",
    "\n",
    "        \n",
    "        self.hedge1, self.hedge2 = action\n",
    "        self.t += 1\n",
    "        done = (self.t == self.n_steps)\n",
    "        self.S1 = self.S1_path[self.t]\n",
    "        self.S2 = self.S2_path[self.t]\n",
    "        self.history.append((self.S1, self.S2, self.hedge1, self.hedge2, self.cash))\n",
    "\n",
    "        state = self._get_state()\n",
    "        reward = 0.0\n",
    "        if done:\n",
    "            \n",
    "            self.cash += self.hedge1 * self.S1 + self.hedge2 * self.S2\n",
    "            basket = self.w1 * self.S1 + self.w2 * self.S2\n",
    "            payoff = max(basket - self.strike, 0)\n",
    "            reward = -(self.cash - payoff)  \n",
    "            self.history.append((self.S1, self.S2, 0.0, 0.0, self.cash))\n",
    "        return state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df24cbb8-a70a-4112-b433-ab82808f2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, mean reward (last 50): 3.4025\n",
      "Episode 0, mean reward (last 50): 3.4025\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 154.91 | 126.66 |  0.793 | -0.349 |  -81.04\n",
      "   2 | 156.53 | 126.53 |  0.750 | -0.354 |  -73.90\n",
      "   3 | 161.75 | 130.52 |  0.713 | -0.373 |  -65.72\n",
      "   4 | 160.04 | 129.76 |  0.680 | -0.400 |  -56.88\n",
      "   5 | 158.38 | 127.98 |  0.638 | -0.402 |  -49.86\n",
      "   6 | 158.38 | 127.98 |  0.000 |  0.000 |   -0.22\n",
      "Episode 1000, mean reward (last 50): 5.1244\n",
      "Episode 1000, mean reward (last 50): 5.1113\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 156.63 | 128.88 | -0.082 | -0.299 |   51.74\n",
      "   2 | 164.31 | 133.69 | -0.051 | -0.248 |   40.35\n",
      "   3 | 167.67 | 135.17 | -0.003 | -0.205 |   26.69\n",
      "   4 | 165.40 | 135.49 |  0.045 | -0.171 |   14.00\n",
      "   5 | 164.57 | 139.39 |  0.089 | -0.144 |    3.08\n",
      "   6 | 164.57 | 139.39 |  0.000 |  0.000 |   -2.33\n",
      "Episode 2000, mean reward (last 50): 5.2060\n",
      "Episode 2000, mean reward (last 50): 4.7691\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 162.28 | 132.27 | -0.112 |  0.076 |    7.97\n",
      "   2 | 161.63 | 131.54 | -0.069 |  0.074 |    1.31\n",
      "   3 | 159.72 | 131.24 | -0.034 |  0.078 |   -5.00\n",
      "   4 | 160.68 | 132.58 | -0.001 |  0.085 |  -11.08\n",
      "   5 | 158.27 | 129.46 |  0.027 |  0.092 |  -16.63\n",
      "   6 | 158.27 | 129.46 |  0.000 |  0.000 |   -0.33\n",
      "Episode 3000, mean reward (last 50): 5.3931\n",
      "Episode 3000, mean reward (last 50): 6.1787\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 158.01 | 131.15 | -0.116 | -0.214 |   46.21\n",
      "   2 | 162.47 | 132.41 | -0.089 | -0.210 |   41.38\n",
      "   3 | 160.62 | 128.36 | -0.077 | -0.220 |   40.72\n",
      "   4 | 157.36 | 127.60 | -0.070 | -0.230 |   40.99\n",
      "   5 | 171.25 | 135.31 | -0.061 | -0.240 |   40.71\n",
      "   6 | 171.25 | 135.31 |  0.000 |  0.000 |   -2.15\n",
      "Episode 4000, mean reward (last 50): 5.4650\n",
      "Episode 4000, mean reward (last 50): 5.9588\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 154.69 | 127.68 |  0.178 | -0.056 |  -20.98\n",
      "   2 | 156.33 | 123.97 |  0.161 | -0.050 |  -19.25\n",
      "   3 | 153.44 | 122.54 |  0.142 | -0.039 |  -17.63\n",
      "   4 | 154.76 | 123.87 |  0.131 | -0.025 |  -17.58\n",
      "   5 | 154.75 | 122.20 |  0.124 | -0.012 |  -18.17\n",
      "   6 | 154.75 | 122.20 |  0.000 |  0.000 |   -0.47\n",
      "Episode 5000, mean reward (last 50): 5.1684\n",
      "Episode 5000, mean reward (last 50): 5.5875\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 154.43 | 124.01 |  0.156 | -0.116 |   -9.86\n",
      "   2 | 157.06 | 127.83 |  0.151 | -0.097 |  -11.40\n",
      "   3 | 155.72 | 124.89 |  0.134 | -0.100 |   -8.34\n",
      "   4 | 158.61 | 122.83 |  0.132 | -0.102 |   -7.86\n",
      "   5 | 160.71 | 124.89 |  0.140 | -0.107 |   -8.52\n",
      "   6 | 160.71 | 124.89 |  0.000 |  0.000 |    0.67\n",
      "Episode 6000, mean reward (last 50): 5.1818\n",
      "Episode 6000, mean reward (last 50): 5.2670\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 157.16 | 131.80 | -0.034 | -0.023 |    8.39\n",
      "   2 | 150.82 | 129.15 | -0.021 | -0.054 |   10.52\n",
      "   3 | 156.31 | 128.73 | -0.003 | -0.076 |   10.56\n",
      "   4 | 151.43 | 120.75 |  0.004 | -0.105 |   13.23\n",
      "   5 | 153.76 | 121.86 |  0.017 | -0.127 |   13.87\n",
      "   6 | 153.76 | 121.86 |  0.000 |  0.000 |    1.03\n",
      "Episode 7000, mean reward (last 50): 5.0540\n",
      "Episode 7000, mean reward (last 50): 5.2280\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 161.50 | 128.70 |  0.372 |  0.083 |  -69.97\n",
      "   2 | 159.27 | 125.93 |  0.337 |  0.063 |  -61.79\n",
      "   3 | 156.35 | 129.99 |  0.304 |  0.045 |  -54.23\n",
      "   4 | 154.42 | 126.87 |  0.280 |  0.034 |  -49.10\n",
      "   5 | 157.25 | 127.05 |  0.247 |  0.007 |  -40.55\n",
      "   6 | 157.25 | 127.05 |  0.000 |  0.000 |   -0.81\n",
      "Episode 8000, mean reward (last 50): 4.7230\n",
      "Episode 8000, mean reward (last 50): 4.0340\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 157.09 | 128.24 | -0.334 |  0.275 |   17.60\n",
      "   2 | 161.07 | 130.86 | -0.310 |  0.246 |   17.63\n",
      "   3 | 162.99 | 133.10 | -0.304 |  0.213 |   20.95\n",
      "   4 | 162.74 | 132.89 | -0.293 |  0.174 |   24.33\n",
      "   5 | 164.94 | 135.58 | -0.279 |  0.139 |   26.66\n",
      "   6 | 164.94 | 135.58 |  0.000 |  0.000 |   -0.51\n",
      "Episode 9000, mean reward (last 50): 5.1859\n",
      "Episode 9000, mean reward (last 50): 4.7428\n",
      "Time | S1   | S2   | hedge1 | hedge2 | cash\n",
      "   0 | 159.26 | 129.41 |  0.000 |  0.000 |    0.00\n",
      "   1 | 155.08 | 126.48 | -0.078 | -0.543 |   82.80\n",
      "   2 | 150.62 | 125.72 | -0.051 | -0.455 |   67.37\n",
      "   3 | 146.63 | 124.04 | -0.024 | -0.363 |   51.83\n",
      "   4 | 148.57 | 123.79 |  0.002 | -0.284 |   38.17\n",
      "   5 | 147.28 | 122.81 |  0.036 | -0.204 |   23.26\n",
      "   6 | 147.28 | 122.81 |  0.000 |  0.000 |    3.41\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n_episodes = 10000  \n",
    "strike = 140\n",
    "all_rewards = []\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "   \n",
    "    start_lags1 = [y1[-7], y1[-6]]  \n",
    "    start_lags2 = [y2[-7], y2[-6]]  \n",
    "    S1_0 = df1['Adjusted Close'].iloc[-6] \n",
    "    S2_0 = df2['Adjusted Close'].iloc[-6]\n",
    "   \n",
    "    S1_path, S2_path = sim_paths_hedge(n_steps, start_lags1, start_lags2, S1_0, S2_0)\n",
    "    \n",
    "    include_prev_hedge = False  \n",
    "\n",
    "    env = BasketHedgingEnv(S1_path, S2_path, strike, w1=0.5, w2=0.5, cost=0.0001, include_prev_hedge=include_prev_hedge)\n",
    "\n",
    "    if include_prev_hedge:\n",
    "        n_inputs = 5  \n",
    "    else:\n",
    "        n_inputs = 3  \n",
    "\n",
    "    policy = BasketPolicyNet(n_inputs=n_inputs, hidden=64)\n",
    "\n",
    "    # 4. Rollout episode\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        action_np = np.clip(action.detach().numpy(), -2, 2)  \n",
    "        state_new, reward, done, _ = env.step(action_np)\n",
    "        log_prob = -((policy(state) - torch.tensor(action_np)).pow(2)).sum()\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        state = torch.tensor(state_new, dtype=torch.float32)\n",
    "    R = sum(rewards)\n",
    "    all_rewards.append(R)\n",
    "    loss = -torch.stack(log_probs).sum() * R\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if ep % 1000 == 0:\n",
    "        print(f\"Episode {ep}, mean reward (last 50): {np.mean(all_rewards[-1000:]):.4f}\")\n",
    "\n",
    "    if ep % 1000 == 0:\n",
    "        print(f\"Episode {ep}, mean reward (last 50): {np.mean(all_rewards[-50:]):.4f}\")\n",
    "        print(\"Time | S1   | S2   | hedge1 | hedge2 | cash\")\n",
    "        for t, (s1, s2, h1, h2, cash) in enumerate(env.history):\n",
    "            print(f\"{t:>4} | {s1:>5.2f} | {s2:>5.2f} | {h1:>6.3f} | {h2:>6.3f} | {cash:>7.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
